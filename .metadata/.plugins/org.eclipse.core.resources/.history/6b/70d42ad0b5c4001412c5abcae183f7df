package mapred;

import java.io.ByteArrayOutputStream;
import java.io.EOFException;
import java.io.IOException;
import java.util.zip.GZIPInputStream;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.io.IOUtils;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.RecordReader;
import org.apache.hadoop.mapreduce.InputSplit;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.hadoop.mapreduce.lib.input.FileSplit;

/**
 * This RecordReader implementation extracts individual files from a ZIP
 * file and hands them over to the Mapper. The "key" is the decompressed
 * file name, the "value" is the file contents.
 */
public class GzipFileRecordReader
    extends RecordReader<Text, BytesWritable>
{
    /** InputStream used to read the GZIP file from the FileSystem */
    private FSDataInputStream fsin;

    /** GZIP file parser/decompresser */
    private GZIPInputStream gzip;

    /** Uncompressed file name */
    private Text currentKey;

    /** Uncompressed file contents */
    private BytesWritable currentValue;

    /** Used to indicate progress */
    private boolean isFinished = false;
    
    private FileSplit fileSplit;

    /**
     * Initialise and open the ZIP file from the FileSystem
     */
    @Override
    public void initialize( InputSplit inputSplit, TaskAttemptContext taskAttemptContext )
        throws IOException, InterruptedException
    {
        this.fileSplit = (FileSplit) inputSplit;
        Configuration conf = taskAttemptContext.getConfiguration();
        
        Path path = fileSplit.getPath();
        currentKey = new Text(path.getName());
        FileSystem fs = path.getFileSystem( conf );
        
        // Open the stream
        fsin = fs.open( path );
        gzip = new GZIPInputStream( fsin );
    }

    /**
     * This is where the magic happens, each ZipEntry is decompressed and
     * readied for the Mapper. The contents of each file is held *in memory*
     * in a BytesWritable object.
     * 
     * If the ZipFileInputFormat has been set to Lenient (not the default),
     * certain exceptions will be gracefully ignored to prevent a larger job
     * from failing.
     */
    @Override
    public boolean nextKeyValue()
        throws IOException, InterruptedException
    {
        long length = fileSplit.getLength();
        byte[] temp = new byte[(int) length];
        
        Path file = fileSplit.getPath();
        currentKey.set(file.getName());
        
        try {
            IOUtils.readFully(gzip, temp, 0, temp.length);
            currentValue.set(temp, 0, temp.length);
        } finally {
            IOUtils.closeStream(gzip);
        }
        
        return true;
    }

    /**
     * Rather than calculating progress, we just keep it simple
     */
    @Override
    public float getProgress()
        throws IOException, InterruptedException
    {
        return isFinished ? 1 : 0;
    }

    /**
     * Returns the current key (name of the zipped file)
     */
    @Override
    public Text getCurrentKey()
        throws IOException, InterruptedException
    {
        return currentKey;
    }

    /**
     * Returns the current value (contents of the zipped file)
     */
    @Override
    public BytesWritable getCurrentValue()
        throws IOException, InterruptedException
    {
        return currentValue;
    }

    /**
     * Close quietly, ignoring any exceptions
     */
    @Override
    public void close()
        throws IOException
    {
        try { gzip.close(); } catch ( Exception ignore ) { }
        try { fsin.close(); } catch ( Exception ignore ) { }
    }
}